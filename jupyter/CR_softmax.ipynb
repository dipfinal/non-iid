{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import gc, argparse, sys, os, errno\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_style('whitegrid')\n",
    "import h5py\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import scipy\n",
    "import sklearn\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chenxupeng/projects\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] implement  https://github.com/Silver-Shen/Causally-Regularized-Learning\n",
    "- [x] test on training data\n",
    "- [ ] multi class\n",
    "- [ ] numba accelarate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncoding(y, numOfClasses):\n",
    "    \"\"\"\n",
    "    Convert a vector into one-hot encoding matrix where that particular column value is 1 and rest 0 for that row.\n",
    "    :param y: Label vector\n",
    "    :param numOfClasses: Number of unique labels\n",
    "    :return: one-hot encoding matrix\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype='int32')\n",
    "    if len(y) > 1:\n",
    "        y = y.reshape(-1)\n",
    "    if not numOfClasses:\n",
    "        numOfClasses = np.max(y) + 1\n",
    "    yMatrix = np.zeros((len(y), numOfClasses))\n",
    "    yMatrix[np.arange(len(y)), y] = 1\n",
    "    return yMatrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 2*np.round(np.random.rand(1000, 20))-1; # 1000 samples and 20 features\n",
    "beta_true = np.ones([20, 1]);\n",
    "Y = (sigmoid(np.dot(X,beta_true))>=0.5).astype('double');\n",
    "lambda0 = 1; #Logistic loss\n",
    "lambda1 = 0.1; #Balancing loss\n",
    "lambda2 = 1; #L_2 norm of sample weight\n",
    "lambda3 = 0; #L_2 norm of beta\n",
    "lambda4 = 0.001; #L_1 norm of bata\n",
    "lambda5 = 1; #Normalization of sample weight\n",
    "MAXITER = 1000;\n",
    "ABSTOL = 1e-3;\n",
    "W_init = np.random.rand(1000, 1);\n",
    "beta_init = 0.5*np.ones([20, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] TODO: softmax more complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def sigmoid(x):\n",
    "#    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    #z = np.dot(X,W)\n",
    "    exp_x = np.exp(x)\n",
    "    sum_exp_x = np.sum(exp_x,axis=1)\n",
    "    sum_exp_x= sum_exp_x.reshape((x.shape[0],1))\n",
    "    sigmoid = exp_x / sum_exp_x\n",
    "    return sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## J_cost\n",
    "- [ ] TODO: softmax have different lambda0 term\n",
    "\n",
    "Calculate the loss function without the non-differentiable part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = W_init\n",
    "beta = beta_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to implement softmax, change lambda0 term with softmax loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "def softmaxEquation(scores):\n",
    "    \"\"\"\n",
    "    It calculates a softmax probability\n",
    "    :param scores: A matrix(wt * input sample)\n",
    "    :return: softmax probability\n",
    "    \"\"\"\n",
    "    scores -= np.max(scores)\n",
    "    prob = (np.exp(scores).T / np.sum(np.exp(scores), axis=1)).T\n",
    "    return prob\n",
    "\n",
    "def computeLoss(x, yMatrix,wt,regStrength):\n",
    "    \"\"\"\n",
    "    It calculates a cross-entropy loss and gradient to update the weights.\n",
    "    :param x: An input sample\n",
    "    :param yMatrix: Label as one-hot encoding\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    numOfSamples = x.shape[0]\n",
    "    scores = np.dot(x, wt)\n",
    "    prob = softmaxEquation(scores)\n",
    "\n",
    "    loss = -np.log(np.max(prob)) * yMatrix\n",
    "    totalLoss = np.sum(loss) / numOfSamples\n",
    "    grad = ((-1 / numOfSamples) * np.dot(x.T, (yMatrix - prob))) \n",
    "    return totalLoss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.51015297734303"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda0*sum((W*W)*(log(1+exp(X@beta))-Y*(X@beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(W,X,Y,beta):\n",
    "    '''\n",
    "    part of J_loss first term\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    Y_cap = sigmoid(X@beta)\n",
    "    loss = (-1/n)*np.sum((W*W)*Y*np.log(Y_cap))\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambda0*sum((W*W)*(np.log(1+np.exp(X@beta))-Y*(X@beta)))\n",
    "@jit\n",
    "def J_cost(W,beta,X,Y,lambda0, lambda1, lambda2, lambda3, lambda5):\n",
    "    return lambda0*cross_entropy_loss(W,X,Y,beta) \\\n",
    "         +lambda1*sum(balance_cost(W,X)) \\\n",
    "         +lambda2*((W*W).T@(W*W)) \\\n",
    "         +lambda3*sum(beta**2) \\\n",
    "         +lambda5*(sum(W*W)-1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## balance cost\n",
    "- [ ] TODO: it seems do not need to change, means than we can adapt this function into any gradient descent problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def balance_cost(W=None,X=None,*args,**kwargs):\n",
    "    m = X.shape[1]\n",
    "    f_x=np.zeros([m,1])\n",
    "    for i in np.arange(0,m):\n",
    "        X_sub=np.copy(X)\n",
    "        X_sub[:,i]=0\n",
    "        I=(X[:,i] > 0).astype('double')+eps\n",
    "        loss=( np.dot( X_sub.T, np.multiply( np.multiply(W,W),I.reshape(-1,1) ) ) ) / (np.dot((np.multiply(W,W)).T,I.reshape(-1,1)))\\\n",
    "            -(np.dot(X_sub.T,(np.multiply((np.multiply(W,W)),(1 - I.reshape(-1,1)))))) / (np.dot((np.multiply(W,W)).T,(1 - I.reshape(-1,1))))\n",
    "        #print (loss.shape)\n",
    "        f_x[i]=np.dot(loss.T,loss)\n",
    "    return f_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_cost(W,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## balance_grad.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def balance_grad(W=None,X=None,*args,**kwargs):\n",
    "    n,m=X.shape\n",
    "\n",
    "    g_w=np.zeros([n,m])\n",
    "    for i in range(0,m):\n",
    "        X_sub = np.copy(X)\n",
    "        X_sub[:,i] = 0 # the ith column is treatment\n",
    "        I = (X[:,i]>0).reshape(-1,1).astype('double')+eps\n",
    "        J1 = (X_sub.T@((W*W)*I.reshape(-1,1)))/((W*W).T@(I.reshape(-1,1))) \\\n",
    "            -(X_sub.T@((W*W)*(1-I).reshape(-1,1)))/((W*W).T@(1-I).reshape(-1,1))\n",
    "        dJ1W = 2*(X_sub.T*((W*I)@np.ones([1,m])).T*((W*W).T@I) \\\n",
    "                  -(X_sub.T@(((W*W)*I)@(W*I).T)))/((W*W).T@I)**2 \\\n",
    "                  -2*(X_sub.T*((W*(1-I))@np.ones([1,m])).T*((W*W).T@(1-I)) \\\n",
    "                  -((X_sub.T@( (W*W) * (1-I) )) @  (W*(1-I) ).T ))/((W*W).T@(1-I))**2\n",
    "        g_w[:,i] = (2 * dJ1W.T @ J1).ravel()\n",
    "\n",
    "    return g_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_grad(W,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prox_l1.m\n",
    "\n",
    "- [ ] TODO: it is also universal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def prox_l1(v=None,lambda_=None,*args,**kwargs):\n",
    "    x=np.fmax(0,v - lambda_) - np.fmax(0,- v - lambda_)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main function\n",
    "\n",
    "- [ ] TODO: careful change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(W,X,Y,beta):\n",
    "    '''\n",
    "    part of J_loss first term\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    Y_cap = sigmoid(X@beta)\n",
    "    loss = (-1/n)*np.sum((W*W)*Y*np.log(Y_cap))\n",
    "    return loss\n",
    "\n",
    "def grad_CE(W,X,Y,Y_cap):\n",
    "    '''\n",
    "    part of J_loss's first term grad\n",
    "    '''\n",
    "    n = X.shape[0]\n",
    "    grad = (1/n)*np.matmul(X.T,(Y_cap-Y))\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] <span class=\"burk\">TODO</span>: \n",
    "grad_beta grad_W要改！要根据grad_CE，对比cross_entropy_loss和原loss改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X.shape[0]\n",
    "def sigmoid(x):\n",
    "    exp_x = np.exp(x)\n",
    "    sum_exp_x = np.sum(exp_x,axis=1)\n",
    "    sum_exp_x= sum_exp_x.reshape((x.shape[0],1))\n",
    "    sigmoid = exp_x / sum_exp_x\n",
    "    return sigmoid\n",
    "\n",
    "lambda0*(-1/n)*np.sum((W*W)*Y*np.log(sigmoid(X@beta))) #NEW ??对比logistic，应该是对的交叉熵\n",
    "\n",
    "\n",
    "lambda0*sum((W*W)*(np.log(1+np.exp(X@beta))-Y*(X@beta))) #Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_beta = lambda0*(((sigmoid(X@beta)-Y)*(W*W)).T@X).T  #Original  手推一下 X^T(y-p)\n",
    "\n",
    "grad_W = 2*lambda0*(np.log(1+np.exp(X@beta))-Y*(X@beta))*W  #Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@jit\n",
    "def mainFunc(X, Y, \\\n",
    "    lambda0, lambda1, lambda2, lambda3, lambda4, lambda5,\\\n",
    "    MAXITER, ABSTOL, W_init, beta_init,paras_save_path=None):\n",
    "\n",
    "    n,m = X.shape\n",
    "    W = W_init\n",
    "    W_prev = np.copy(W)\n",
    "    beta = beta_init\n",
    "    beta_prev = np.copy(beta)\n",
    "\n",
    "    parameter_iter = 0.5\n",
    "    J_loss = np.ones([MAXITER, 1])*(-1)\n",
    "\n",
    "    lambda_W = 1\n",
    "    lambda_beta = 1\n",
    "\n",
    "    W_All = np.zeros([n, MAXITER])\n",
    "    beta_All = np.zeros([m, MAXITER])\n",
    "\n",
    "\n",
    "    # Optimization with gradient descent\n",
    "    for iter in tqdm(range(1,MAXITER+1)):\n",
    "        # Update beta\n",
    "        y = np.copy(beta)\n",
    "        beta = beta + (iter/(iter+3))*(beta-beta_prev) # fast proximal gradient\n",
    "        f_base = J_cost(W, beta, X, Y, lambda0, lambda1, lambda2, lambda3, lambda5)\n",
    "        grad_beta = lambda0*(((sigmoid(X@beta)-Y)*(W*W)).T@X).T \\\n",
    "                   +2*lambda3*beta\n",
    "\n",
    "        while 1:\n",
    "            z = prox_l1(beta - lambda_beta*grad_beta, lambda_beta*lambda4)\n",
    "            if J_cost(W, z, X, Y, lambda0, lambda1, lambda2, lambda3, lambda5)\\\n",
    "               <= f_base + grad_beta.T@(z-beta)\\\n",
    "               + (1/(2*lambda_beta))*sum((z-beta)**2):\n",
    "                break\n",
    "            lambda_beta = parameter_iter*lambda_beta\n",
    "            if lambda_beta<eps*eps*eps:\n",
    "                break\n",
    "        beta_prev = y\n",
    "        beta = z\n",
    "\n",
    "        # Update W\n",
    "        y = np.copy(W)\n",
    "        W = W+(iter/(iter+3))*(W-W_prev)\n",
    "        f_base = J_cost(W, beta, X, Y, lambda0, lambda1, lambda2, lambda3, lambda5)\n",
    "\n",
    "        grad_W = 2*lambda0*(np.log(1+np.exp(X@beta))-Y*(X@beta))*W \\\n",
    "                +lambda1*balance_grad(W, X)@np.ones([m,1]) \\\n",
    "                +4*lambda2*W*W*W \\\n",
    "                +4*lambda5*(sum(W*W)-1)*W\n",
    "\n",
    "        while 1:\n",
    "            z = prox_l1(W-lambda_W*grad_W, 0)\n",
    "            #print (J_cost(z, beta, X, Y, lambda0, lambda1, lambda2, lambda3, lambda5),\n",
    "            #f_base , grad_W.T@(z-W),(1/(2*lambda_W))*sum((z-W)**2))\n",
    "            if J_cost(z, beta, X, Y, lambda0, lambda1, lambda2, lambda3, lambda5)\\\n",
    "                    <= f_base + grad_W.T@(z-W)\\\n",
    "                    + (1/(2*lambda_W+10e-100))*sum((z-W)**2):\n",
    "                break\n",
    "            lambda_W = parameter_iter*lambda_W\n",
    "            if lambda_W<eps*eps*eps:\n",
    "                break\n",
    "\n",
    "        W_prev = y\n",
    "        W = z\n",
    "\n",
    "        W_All[:,iter-1] = W.ravel()\n",
    "        beta_All[:,iter-1] = beta.ravel()\n",
    "\n",
    "        J_loss[iter-1] = J_cost(W, beta, X, Y,\\\n",
    "                              lambda0, lambda1, lambda2, lambda3, lambda5)\\\n",
    "                     + lambda4*sum(abs(beta))\n",
    "        print (J_loss[iter-1] , J_loss[iter-2])\n",
    "        if (paras_save_path is not None) & (iter%10==1):\n",
    "            #'output/models/crlr/somedir'\n",
    "            if not os.path.exists(paras_save_path):\n",
    "                os.makedirs(paras_save_path)\n",
    "            np.savetxt(paras_save_path+'/beta.txt',beta)\n",
    "            np.savetxt(paras_save_path+'/W.txt',W)\n",
    "            np.savetxt(paras_save_path+'/J_loss.txt',J_loss)\n",
    "        if (iter > 1) & ( abs(J_loss[iter-1] - J_loss[iter-2])[0]  < ABSTOL) or (iter == MAXITER):\n",
    "            break\n",
    "    W = W*W\n",
    "\n",
    "    return W, beta, J_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4ea3db4caf4f9f9a5656ff902a1530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score, precision_score, \\\n",
    "    roc_curve, precision_recall_curve, average_precision_score, matthews_corrcoef, confusion_matrix\n",
    "\n",
    "def report_metrics(y_test, y_pred):\n",
    "    scorers = {'accuracy': accuracy_score,\n",
    "           'recall': recall_score,\n",
    "           'precision': precision_score,\n",
    "           'f1': f1_score,\n",
    "           'mcc': matthews_corrcoef\n",
    "    }\n",
    "    for metric in scorers.keys():\n",
    "        print('{} = {}'.format(metric, scorers[metric](y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 2*np.round(np.random.rand(1000, 20))-1; # 1000 samples and 20 features\n",
    "beta_true = np.ones([20, 1]);\n",
    "Y = (sigmoid(np.dot(X,beta_true))>=0.5).astype('double');\n",
    "lambda0 = 1; #Logistic loss\n",
    "lambda1 = 0.1; #Balancing loss\n",
    "lambda2 = 1; #L_2 norm of sample weight\n",
    "lambda3 = 0; #L_2 norm of beta\n",
    "lambda4 = 0.001; #L_1 norm of bata\n",
    "lambda5 = 1; #Normalization of sample weight\n",
    "MAXITER = 1000;\n",
    "ABSTOL = 1e-3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples: 750, test samples: 250\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y.astype('int'))\n",
    "print('number of training samples: {}, test samples: {}'.format(X_train.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "report_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02742b6d3f14f4e92a0938dfabce586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W_init = np.random.rand(X_train.shape[0], 1);\n",
    "beta_init = 0.5*np.ones([20, 1]);\n",
    "\n",
    "W, beta, J_loss = mainFunc(X_train, y_train,\\\n",
    "        lambda0, lambda1, lambda2, lambda3, lambda4, lambda5,\\\n",
    "        1000, ABSTOL, W_init, beta_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 1.0\n",
      "recall = 1.0\n",
      "precision = 1.0\n",
      "f1 = 1.0\n",
      "mcc = 1.0\n"
     ]
    }
   ],
   "source": [
    "y_pred = (sigmoid(np.dot(X_test,beta))>=0.5).astype('int')\n",
    "report_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230.724px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
