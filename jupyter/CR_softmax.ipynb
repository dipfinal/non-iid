{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import gc, argparse, sys, os, errno\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_style('whitegrid')\n",
    "import h5py\n",
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import scipy\n",
    "import sklearn\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chenxupeng/projects/DIP\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] implement  https://github.com/Silver-Shen/Causally-Regularized-Learning\n",
    "- [x] test on training data\n",
    "- [ ] multi class\n",
    "- [ ] numba accelarate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncoding(y, numOfClasses):\n",
    "    \"\"\"\n",
    "    Convert a vector into one-hot encoding matrix where that particular column value is 1 and rest 0 for that row.\n",
    "    :param y: Label vector\n",
    "    :param numOfClasses: Number of unique labels\n",
    "    :return: one-hot encoding matrix\n",
    "    \"\"\"\n",
    "    y = np.asarray(y, dtype='int32')\n",
    "    if len(y) > 1:\n",
    "        y = y.reshape(-1)\n",
    "    if not numOfClasses:\n",
    "        numOfClasses = np.max(y) + 1\n",
    "    yMatrix = np.zeros((len(y), numOfClasses))\n",
    "    yMatrix[np.arange(len(y)), y] = 1\n",
    "    return yMatrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 2*np.round(np.random.rand(1000, 20))-1; # 1000 samples and 20 features\n",
    "beta_true = np.ones([20, 1]);\n",
    "Y = (sigmoid(np.dot(X,beta_true))>=0.5).astype('double');\n",
    "lambda0 = 1; #Logistic loss\n",
    "lambda1 = 0.1; #Balancing loss\n",
    "lambda2 = 1; #L_2 norm of sample weight\n",
    "lambda3 = 0; #L_2 norm of beta\n",
    "lambda4 = 0.001; #L_1 norm of bata\n",
    "lambda5 = 1; #Normalization of sample weight\n",
    "MAXITER = 1000;\n",
    "ABSTOL = 1e-3;\n",
    "W_init = np.random.rand(1000, 1);\n",
    "beta_init = 0.5*np.ones([20, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## J_cost\n",
    "Calculate the loss function without the non-differentiable part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```matlab\n",
    "function f_x = J_cost(W, beta, X, Y, ...\n",
    "                      lambda0, lambda1, lambda2, lambda3, lambda5)\n",
    "\n",
    "    f_x = lambda0*sum((W.*W).*(log(1+exp(X*beta))-Y.*(X*beta)))...\n",
    "         +lambda1*sum(balance_cost(W,X))...\n",
    "         +lambda2*((W.*W)'*(W.*W))...\n",
    "         +lambda3*sum(beta.^2)...         \n",
    "         +lambda5*(sum(W.*W)-1)^2;\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = W_init\n",
    "beta = beta_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.51015297734303"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda0*sum((W*W)*(np.log(1+np.exp(X@beta))-Y*(X@beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to implement softmax, change lambda0 term with softmax loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxEquation(scores):\n",
    "    \"\"\"\n",
    "    It calculates a softmax probability\n",
    "    :param scores: A matrix(wt * input sample)\n",
    "    :return: softmax probability\n",
    "    \"\"\"\n",
    "    scores -= np.max(scores)\n",
    "    prob = (np.exp(scores).T / np.sum(np.exp(scores), axis=1)).T\n",
    "    return prob\n",
    "\n",
    "def computeLoss(x, yMatrix,wt,regStrength):\n",
    "    \"\"\"\n",
    "    It calculates a cross-entropy loss with regularization loss and gradient to update the weights.\n",
    "    :param x: An input sample\n",
    "    :param yMatrix: Label as one-hot encoding\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    numOfSamples = x.shape[0]\n",
    "    scores = np.dot(x, wt)\n",
    "    prob = softmaxEquation(scores)\n",
    "\n",
    "    loss = -np.log(np.max(prob)) * yMatrix\n",
    "    regLoss = (1/2)*regStrength*np.sum(wt*wt)\n",
    "    totalLoss = (np.sum(loss) / numOfSamples) + regLoss\n",
    "    grad = ((-1 / numOfSamples) * np.dot(x.T, (yMatrix - prob))) + (regStrength * wt)\n",
    "    return totalLoss, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X\n",
    "n,m=X.shape\n",
    "numOfClasses = 10\n",
    "y_10 = np.random.randint(0,10,size=1000)\n",
    "yMatrix = oneHotEncoding(y_10, numOfClasses)\n",
    "regStrength = 0.1\n",
    "\n",
    "wt = wt = 0.001 * np.random.rand(m, numOfClasses)\n",
    "numOfSamples = x.shape[0]\n",
    "scores = np.dot(x, wt)\n",
    "prob = softmaxEquation(scores)\n",
    "\n",
    "loss = -np.log(np.max(prob)) * yMatrix\n",
    "regLoss = (1/2)*regStrength*np.sum(wt*wt)\n",
    "totalLoss = (np.sum(loss) / numOfSamples) + regLoss\n",
    "grad = ((-1 / numOfSamples) * np.dot(x.T, (yMatrix - prob))) + (regStrength * wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 10)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.51015297734303"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda0*sum((W*W)*(log(1+exp(X@beta))-Y*(X@beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_cost(W,beta,X,Y,lambda0, lambda1, lambda2, lambda3, lambda5):\n",
    "    return lambda0*sum((W*W)*(log(1+exp(X@beta))-Y*(X@beta))) \\\n",
    "         +lambda1*sum(balance_cost(W,X)) \\\n",
    "         +lambda2*((W*W).T@(W*W)) \\\n",
    "         +lambda3*sum(beta**2) \\\n",
    "         +lambda5*(sum(W*W)-1)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## balance cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "function f_x = balance_cost(W, X)\n",
    "    m = size(X, 2); % feature number\n",
    "    f_x = zeros(m,1);\n",
    "    for i=1:m\n",
    "        X_sub = X;\n",
    "        X_sub(:,i) = 0; % the ith column is treatment\n",
    "        I = double(X(:,i)>0);\n",
    "        loss = (X_sub'*((W.*W).*I))/((W.*W)'*I)...\n",
    "              -(X_sub'*((W.*W).*(1-I)))/((W.*W)'*(1-I));       \n",
    "        f_x(i) = loss'*loss;\n",
    "    end    \n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_cost(W=None,X=None,*args,**kwargs):\n",
    "    m = X.shape[1]  \n",
    "    f_x=np.zeros([m,1])\n",
    "    for i in np.arange(0,m):\n",
    "        X_sub=copy(X)\n",
    "        X_sub[:,i]=0\n",
    "        I=(X[:,i] > 0).astype('double')+10e-4\n",
    "        loss=( dot( X_sub.T, multiply( multiply(W,W),I.reshape(-1,1) ) ) ) / (dot((multiply(W,W)).T,I.reshape(-1,1)))\\\n",
    "            -(dot(X_sub.T,(multiply((multiply(W,W)),(1 - I.reshape(-1,1)))))) / (dot((multiply(W,W)).T,(1 - I.reshape(-1,1))))\n",
    "        #print (loss.shape)\n",
    "        f_x[i]=dot(loss.T,loss)\n",
    "    return f_x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_cost(W,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## balance_grad.m\n",
    "```\n",
    "function g_w = balance_grad(W, X)\n",
    "    n = size(X, 1); % sample number\n",
    "    m = size(X, 2); % feature number\n",
    "    g_w = zeros(n, m);\n",
    "    for i=1:m\n",
    "        X_sub = X;\n",
    "        X_sub(:,i) = 0; % the ith column is treatment\n",
    "        I = double(X(:,i)>0);\n",
    "        J1 = (X_sub'*((W.*W).*I))/((W.*W)'*I)...\n",
    "            -(X_sub'*((W.*W).*(1-I)))/((W.*W)'*(1-I));\n",
    "        dJ1W = 2*(X_sub'.*((W.*I)*ones(1,m))'*((W.*W)'*I)...\n",
    "                  -X_sub'*((W.*W).*I)*(W.*I)')/((W.*W)'*I)^2 ...\n",
    "              -2*(X_sub'.*((W.*(1-I))*ones(1,m))'*((W.*W)'*(1-I))...\n",
    "                  -X_sub'*((W.*W).*(1-I))*(W.*(1-I))')/((W.*W)'*(1-I))^2;\n",
    "        g_w(:,i) = 2*dJ1W'*J1;\n",
    "    end\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_grad(W=None,X=None,*args,**kwargs):\n",
    "    n,m=X.shape\n",
    "    \n",
    "    g_w=np.zeros([n,m])\n",
    "    for i in range(0,m):\n",
    "        X_sub = X;\n",
    "        X_sub[:,i] = 0; # the ith column is treatment\n",
    "        I = (X[:,i]>0).reshape(-1,1).astype('double')+10e-4;\n",
    "        J1 = (X_sub.T@((W*W)*I.reshape(-1,1)))/((W*W).T@(I.reshape(-1,1))) \\\n",
    "            -(X_sub.T@((W*W)*(1-I).reshape(-1,1)))/((W*W).T@(1-I).reshape(-1,1));\n",
    "        dJ1W = 2*(X_sub.T*((W*I)@np.ones([1,m])).T*((W*W).T@I) \\\n",
    "                  -(X_sub.T@(((W*W)*I)@(W*I).T)))/((W*W).T@I)**2 \\\n",
    "                  -2*(X_sub.T*((W*(1-I))@np.ones([1,m])).T*((W*W).T@(1-I)) \\\n",
    "                  -((X_sub.T@( (W*W) * (1-I) )) @  (W*(1-I) ).T ))/((W*W).T@(1-I))**2;\n",
    "        g_w[:,i] = (2 * dJ1W.T @ J1).ravel();\n",
    "    \n",
    "    return g_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balance_grad(W,X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prox_l1.m\n",
    "```\n",
    "function x = prox_l1(v, lambda)\n",
    "% PROX_L1    The proximal operator of the l1 norm.\n",
    "%\n",
    "%   prox_l1(v,lambda) is the proximal operator of the l1 norm\n",
    "%   with parameter lambda.\n",
    "% max: compare with a scalar and return element wise bigger value\n",
    "\n",
    "    x = max(0, v - lambda) - max(0, -v - lambda);\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prox_l1(v=None,lambda_=None,*args,**kwargs):\n",
    "    x=np.fmax(0,v - lambda_) - np.fmax(0,- v - lambda_)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "function [W, beta, J_loss] = mainFunc(X, Y, ...\n",
    "    lambda0, lambda1, lambda2, lambda3, lambda4, lambda5,...\n",
    "    MAXITER, ABSTOL, W_init, beta_init)\n",
    "\n",
    "%% Initialization\n",
    "n = size(X, 1); % Sample size\n",
    "m = size(X, 2); % Feature dimension\n",
    "W = W_init;\n",
    "W_prev = W;\n",
    "beta = beta_init;\n",
    "beta_prev = beta;\n",
    "\n",
    "parameter_iter = 0.5;\n",
    "J_loss = ones(MAXITER, 1)*(-1);\n",
    "\n",
    "lambda_W = 1;\n",
    "lambda_beta = 1;\n",
    "\n",
    "W_All = zeros(n, MAXITER);\n",
    "beta_All = zeros(m, MAXITER);\n",
    "\n",
    "%% Optimization with gradient descent\n",
    "for iter = 1:MAXITER\n",
    "    % Update beta\n",
    "    y = beta;\n",
    "    beta = beta + (iter/(iter+3))*(beta-beta_prev); % fast proximal gradient\n",
    "    f_base = J_cost(W, beta, X, Y, lambda0, lambda1, lambda2, lambda3, lambda5);\n",
    "    grad_beta = lambda0*(((sigmoid(X*beta)-Y).*(W.*W))'*X)'...               \n",
    "               +2*lambda3*beta;\n",
    "    \n",
    "    while 1\n",
    "        z = prox_l1(beta - lambda_beta*grad_beta, lambda_beta*lambda4);\n",
    "        if J_cost(W, z, X, Y, lambda0, lambda1, lambda2, lambda3, lambda5)...\n",
    "           <= f_base + grad_beta'*(z-beta) ...\n",
    "           + (1/(2*lambda_beta))*sum((z-beta).^2)\n",
    "            break;\n",
    "        end\n",
    "        lambda_beta = parameter_iter*lambda_beta;\n",
    "    end\n",
    "    beta_prev = y;\n",
    "    beta = z;\n",
    "    \n",
    "    % Update W\n",
    "    y = W;\n",
    "    W = W+(iter/(iter+3))*(W-W_prev);    \n",
    "    f_base = J_cost(W, beta, X, Y, lambda0, lambda1, lambda2, lambda3, lambda5);\n",
    "    \n",
    "    grad_W = 2*lambda0*(log(1+exp(X*beta))-Y.*(X*beta)).*W...\n",
    "            +lambda1*balance_grad(W, X)*ones(m,1)...\n",
    "            +4*lambda2*W.*W.*W...           \n",
    "            +4*lambda5*(sum(W.*W)-1)*W;\n",
    "        \n",
    "    while 1\n",
    "        z = prox_l1(W-lambda_W*grad_W, 0);\n",
    "        if J_cost(z, beta, X, Y, lambda0, lambda1, lambda2, lambda3, lambda5)...\n",
    "                <= f_base + grad_W'*(z-W) ...\n",
    "                + (1/(2*lambda_W))*sum((z-W).^2)\n",
    "            break;\n",
    "        end\n",
    "        lambda_W = parameter_iter*lambda_W;\n",
    "    end\n",
    "    W_prev = y;\n",
    "    W = z;    \n",
    "    \n",
    "    W_All(:,iter) = W;\n",
    "    beta_All(:,iter) = beta;\n",
    "    \n",
    "    J_loss(iter) = J_cost(W, beta, X, Y, ....\n",
    "                          lambda0, lambda1, lambda2, lambda3, lambda5)...\n",
    "                 + lambda4*sum(abs(beta));\n",
    "             \n",
    "    if iter > 1 && abs(J_loss(iter) - J_loss(iter-1)) < ABSTOL || iter == MAXITER\n",
    "        break\n",
    "    end   \n",
    "end    \n",
    "W = W.*W;\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainFunc(X, Y, \\\n",
    "    lambda0, lambda1, lambda2, lambda3, lambda4, lambda5,\\\n",
    "    MAXITER, ABSTOL, W_init, beta_init):\n",
    "    \n",
    "    n,m = X.shape\n",
    "    W = W_init;\n",
    "    W_prev = W;\n",
    "    beta = beta_init;\n",
    "    beta_prev = beta;\n",
    "\n",
    "    parameter_iter = 0.5;\n",
    "    J_loss = np.ones([MAXITER, 1])*(-1);\n",
    "\n",
    "    lambda_W = 1;\n",
    "    lambda_beta = 1;\n",
    "\n",
    "    W_All = np.zeros([n, MAXITER]);\n",
    "    beta_All = np.zeros([m, MAXITER]);\n",
    "\n",
    "\n",
    "    # Optimization with gradient descent\n",
    "    for iter in tqdm(range(1,MAXITER+1)):\n",
    "        # Update beta\n",
    "        y = beta;\n",
    "        beta = beta + (iter/(iter+3))*(beta-beta_prev); # fast proximal gradient\n",
    "        f_base = J_cost(W, beta, X, Y, lambda0, lambda1, lambda2, lambda3, lambda5);\n",
    "        grad_beta = lambda0*(((sigmoid(X@beta)-Y)*(W*W)).T@X).T \\\n",
    "                   +2*lambda3*beta;\n",
    "\n",
    "        while 1:\n",
    "            z = prox_l1(beta - lambda_beta*grad_beta, lambda_beta*lambda4);\n",
    "            if J_cost(W, z, X, Y, lambda0, lambda1, lambda2, lambda3, lambda5)\\\n",
    "               <= f_base + grad_beta.T@(z-beta)\\\n",
    "               + (1/(2*lambda_beta))*sum((z-beta)**2):\n",
    "                break;\n",
    "            lambda_beta = parameter_iter*lambda_beta;\n",
    "        beta_prev = y;\n",
    "        beta = z;\n",
    "\n",
    "        # Update W\n",
    "        y = W;\n",
    "        W = W+(iter/(iter+3))*(W-W_prev);    \n",
    "        f_base = J_cost(W, beta, X, Y, lambda0, lambda1, lambda2, lambda3, lambda5);\n",
    "\n",
    "        grad_W = 2*lambda0*(log(1+exp(X@beta))-Y*(X@beta))*W \\\n",
    "                +lambda1*balance_grad(W, X)@np.ones([m,1]) \\\n",
    "                +4*lambda2*W*W*W \\\n",
    "                +4*lambda5*(sum(W*W)-1)*W;\n",
    "\n",
    "        while 1:\n",
    "            z = prox_l1(W-lambda_W*grad_W, 0);\n",
    "            if J_cost(z, beta, X, Y, lambda0, lambda1, lambda2, lambda3, lambda5)\\\n",
    "                    <= f_base + grad_W.T@(z-W)\\\n",
    "                    + (1/(2*lambda_W))*sum((z-W)**2):\n",
    "                break;\n",
    "            lambda_W = parameter_iter*lambda_W;\n",
    "        W_prev = y;\n",
    "        W = z;    \n",
    "\n",
    "        W_All[:,iter-1] = W.ravel();\n",
    "        beta_All[:,iter-1] = beta.ravel();\n",
    "\n",
    "        J_loss[iter-1] = J_cost(W, beta, X, Y,\\\n",
    "                              lambda0, lambda1, lambda2, lambda3, lambda5)\\\n",
    "                     + lambda4*sum(abs(beta));\n",
    "\n",
    "        if (iter > 1) & ( abs(J_loss[iter-1] - J_loss[iter-2])[0]  < ABSTOL) or (iter == MAXITER):\n",
    "            break\n",
    "    W = W*W;\n",
    "    \n",
    "    return W, beta, J_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4ea3db4caf4f9f9a5656ff902a1530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score, precision_score, \\\n",
    "    roc_curve, precision_recall_curve, average_precision_score, matthews_corrcoef, confusion_matrix\n",
    "\n",
    "def report_metrics(y_test, y_pred):\n",
    "    scorers = {'accuracy': accuracy_score,\n",
    "           'recall': recall_score,\n",
    "           'precision': precision_score,\n",
    "           'f1': f1_score,\n",
    "           'mcc': matthews_corrcoef\n",
    "    }\n",
    "    for metric in scorers.keys():\n",
    "        print('{} = {}'.format(metric, scorers[metric](y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 2*np.round(np.random.rand(1000, 20))-1; # 1000 samples and 20 features\n",
    "beta_true = np.ones([20, 1]);\n",
    "Y = (sigmoid(np.dot(X,beta_true))>=0.5).astype('double');\n",
    "lambda0 = 1; #Logistic loss\n",
    "lambda1 = 0.1; #Balancing loss\n",
    "lambda2 = 1; #L_2 norm of sample weight\n",
    "lambda3 = 0; #L_2 norm of beta\n",
    "lambda4 = 0.001; #L_1 norm of bata\n",
    "lambda5 = 1; #Normalization of sample weight\n",
    "MAXITER = 1000;\n",
    "ABSTOL = 1e-3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples: 750, test samples: 250\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,Y.astype('int'))\n",
    "print('number of training samples: {}, test samples: {}'.format(X_train.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "report_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02742b6d3f14f4e92a0938dfabce586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "W_init = np.random.rand(X_train.shape[0], 1);\n",
    "beta_init = 0.5*np.ones([20, 1]);\n",
    "\n",
    "W, beta, J_loss = mainFunc(X_train, y_train,\\\n",
    "        lambda0, lambda1, lambda2, lambda3, lambda4, lambda5,\\\n",
    "        1000, ABSTOL, W_init, beta_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 1.0\n",
      "recall = 1.0\n",
      "precision = 1.0\n",
      "f1 = 1.0\n",
      "mcc = 1.0\n"
     ]
    }
   ],
   "source": [
    "y_pred = (sigmoid(np.dot(X_test,beta))>=0.5).astype('int')\n",
    "report_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "230.724px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
